## Describe some recent developments in Artificial Intelligence (AI) and evaluate some of the ways of testing its reliability

In August 1955, Artificial Intelligence was first mentioned in the Dartmouth Summer Research Project and defined as a machine behaves like a human being (McCarthy *et al.* 1955, p. 9). In fact, AI performs well in the lab, but may be overwhelmed when it encounters unknown data. Therefore, it is necessary to explore the reliability of AI. This paper aims to evaluate several commonly used methods for determining AI reliability, such as the Turing tests, threshold metrics, probability metrics, and ranking metrics. Although the threshold is a method frequently used by researchers, this paper suggests, at present, ranking metrics are more suitable for assessing AI reliability. In this study, recent developments of AI in different fields will be reviewed. Further, methods commonly used for testing AI will be interpreted and evaluated. Finally, conclusions and limitations will be given.

### Description of AI Developments

Artificial intelligence has flourished and promoted a variety of "hot" research areas, including large-scale machine learning, deep learning and collaborative systems, etc (Peter Stone, 2016, p. 9). However, the most attractive and valuable part is the application of AI in real life, such as finance, services and agriculture.

**Finance:** Owing to online trading and massive data collection, stock market forecasting has become possible. Specifically, AI is now able to predict future trends by learning stock market data. Recently, an expert financial system designed by Weng, Ahmed and Megahed (2017), through learning stock market sequences and "knowledge bases", can successfully predict the AAPL stock movement in the one-day ahead with an accuracy of 85%. While the algorithm may be correct, their study is incomplete because the reliability of the data sources is suspect. This is because everyone can add information to Google and Wikipedia. Using such data may introduce false data and affect AI performance, or even produce the opposite results compared with the ground truth. Batra and Daudpota argue despite Weng, Ahmed and Megahed's accuracy of 85%, they don't verify the external data, such as the data collected from Google or Wikipedia which can be unsubstantiated (Batra and Daudpota, 2018, p. 2). Thus, although the stock market forecasting algorithm can achieve higher accuracy, a more reliable stock market database needs to be collected. **Services:** AI also has great potential in the service industry, such as virtual voice assistants. In 2016, Oord et al. proposed the WaveNet, which not only used to imitate speech, but also mimics the quality of sound and recording, as well as the breathing and mouth movements of the speaker (Oord *et al.*, 2016). Combined with the above technologies, Google Duplex released by Google can make calls with users, and successfully book haircut services as well as call the restaurant to make dinner reservations. It sounds like a real person, with the ability to use "ums" and "ahs" or pause at the right time. When the conversation does not reach expectation, it can also respond deftly (Wong, 2018, p. 21). **Agriculture:** Agriculture can also become more efficient with the help of AI. According to the state-of-art technology, Iraji assists agricultural production by designing high-accuracy intelligent systems using different artificial intelligence methods. Her well-designed intelligent system can accurately score and classify tomato quality, achieving 95.5% accuracy, and performance is much better than other algorithms (Iraji, 2019).

AI has unprecedented opportunity in a variety of different fields, just as Peter Stone mentions, including healthcare, transportation and education, etc (Peter Stone, 2016, p. 6).  However, it also full of challenges. In some specific areas, AI mistakes can lead to severe consequences. Whether it is the misdiagnosis of the diseases or the failure of pedestrian recognition in autonomous driving, it is very likely to lead to life-threatening medical accidents or car accidents. Japkowicz mentions that unstable AI in highly sensitive tasks would behave in a fairly non-compliant manner (Japkowicz, 2006, p. 5). Therefore, it is necessary to explore the existing methods of testing AI reliability. 

### Evaluation of Testing Methods

As discussed in the previous section, the methods of testing AI is crucial. So next, different AI test methods will be evaluated, from the well-known Turing test to three different types of evaluation metrics commonly used by researchers: threshold metrics, probability metrics, and ranking metrics.

#### Turing Test

The Turing test does not necessarily test the reliability of the AI. There is not a measurable metrics which could assess the AI of a machine. It seems possible that these results are due to the initial aim of the Turing Test is to imitate human rather than evaluating the quality of AI. Initially, Turing raised a question of whether machines can think and tries to convert this problem in a more concrete way and proposed an imitation game (Turing, 1950). However, the Turing test has not been quantified and scored for AI reliability. Instead, he concentrates more on whether a machine can act like humans. Moor also asserts that the Turing test has little value in the guidance of research (Moor, 1976, p. 256).

#### Evaluation Metrics

Evaluation metrics are suitable for assessing the reliability of an intelligent system. According to Hossin and Sulaiman, the reliability of AI can be measured by the evaluation metrics (Hossin and Sulaiman, 2015, p. 1). To more easily compare the performance of various evaluation metrics, in Caruana and Niculescu-Mizil's complete research, nine common evaluation metrics are astutely split into precise three categories: threshold metrics, probability metrics, and rank metrics (Caruana and Niculescu-Mizil, 2004, p. 70).

**Threshold Metrics:** Accuracy as one of the popular threshold metrics is often used as the gold standard for researchers. More and more researchers use it as a standard measurement for evaluating AI models because of its accessibility. As Hossin and Sulaiman observe, the threshold and ranking metric are the methods most commonly used by researchers to measure the ability of AI when dealing with new data (Hossin and Sulaiman, 2015, p. 3). In Japkowicz’s review of the accuracy, he claims that researchers compare different intelligent systems based on accuracy. This metrics is the most straightforward and most direct evaluation measure since it counts the number of mistakes which an intelligent system makes (Japkowicz, 2006, p. 2). However, there is a limit to only use accuracy as an evaluation criterion. Provost, Fawcett and Kohavi bring up severe concerns about the use of accuracy. In their considerable work, they point out that accuracy is unsatisfactory both for making practical choices and for drawing scientific conclusions (Provost, Fawcett and Kohavi, 2011, p. 1). Japkowicz mentions another argument for the accuracy's assessment ability is limited. He emphasizes the accuracy cannot distinguish the types of errors made by the AI system, and in some cases ignoring the problem can lead to catastrophic consequences, such as cancer discrimination (Japkowicz, 2006, p. 2). Thus, accuracy cannot be the only metric to evaluate the testing reliability of the AI system.

**Probability Metrics:** The probability metrics are mainly determined by the predicted value. It computes the most direct gap between the ground truth and AI's prediction. According to Liu et al.'s research, probability metrics are extensively used in regression problems by calculating the bias and can be used to evaluate the reliability of intelligent systems. However, they also point out that unlike other metrics, it cannot be compared directly to threshold metrics nor the ranking metrics (Liu *et al.*, 2014, p. 25). Probability metrics can still not adequately handle the task of assessing reliability. In Hossin and Sulaiman's case study of probability metrics, they state that like the defect of accuracy, the probability metrics cannot provide information between different classes. They also wisely realized that the probability metrics depends in no small extent on the weight of the initialization (Hossin and Sulaiman, 2015, p. 5). 

**Ranking Metrics:** In ranking metrics, the area under the curve (AUC) is often used to evaluate the stability of intelligent systems. It's not as simple as accuracy, and it reflects the overall performance of the system. In his thoughtful research, Flach points out that it makes sense to use AUC as a comprehensive performance indicator (Flach, 2019, p. 2). Similarly, Japkowicz emphasizes, unlike other metrics, its great advantage is that it separates the algorithm’s performance of each class, avoiding the shortcomings of the two evaluation metrics mentioned above (Japkowicz, 2006, p. 4). Additionally, Hossin and Sulaiman suggest AUC reflects the overall performance of the AI system. Their perspective is analogous to Japkowicz. However, they also accept that although AUC is very good at evaluation and discrimination, the cost of calculating AUC is too high (Hossin and Sulaiman, 2015, p. 5). Nevertheless, considering its excellent ability in evaluating the reliability of an AI system, in today’s era of almost doubling computing power every two years (Nambiar and Posses, 2010, p. 120), such high computing consumption is affordable.

**Limitation:** The measurement of AI system reliability may need to be further explored. In Flach's research, he suggests the ability of an AI system are not likely to be observed directly (Flach, 2019, p. 1). Additionally, a single evaluation metric may not be sufficient to assess AI. Liu's finding shows that excellent AI systems do not necessarily need to achieve the best in every evaluation metric, as long as the performance requirement of an application is met by the system (Liu *et al.* 2014, p. 30). Thus, this may imply that a single evaluation metric is not reliable enough, but the combination of various evaluation metrics may help test AI reliability. However, this need for further exploration.

### Conclusion

In conclusion, these results suggest that AUC as one of the most used ranking metrics might be more suitable for testing AI reliability at present. This essay objectively analyses the Turing test as well as three types of assessment metrics used for AI reliability testing. Their advantages, disadvantages and reliability are analysed separately. Because these methods may not cover all aspects of testing the reliability of an intelligent system, the methods of testing AI reliability still needs further investigation. 





### References

Batra, R. and Daudpota, S. M., (2018). Integrating StockTwits with sentiment analysis for better prediction of stock price movement. In: KDD-2004. *2018 International Conference on Computing, Mathematics and Engineering Technologies (iCoMET 2018), 3-4 March 2018, Sukkur, Pakistan* [online]. Sukkur: Sukkur IBA Journal of Computing and Mathematical Sciences (SJCMS). pp. 1–5. [Viewed 15 August 2019]. Available from: doi: 10.1109/ICOMET.2018.8346382

Caruana, R. and Niculescu-Mizil, A., (2004). Data mining in metric space: An empirical analysis of supervised learning performance criteria. *10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 22-25 August 2004, Seattle, WA, USA* [online]. New York: Association for Computing Machinery. pp. 69–78. [Viewed 15 August 2019]. Available from: doi: 10.1145/1014052.1014063

Caruana, R. and Niculescu-Mizil, A., (2006). An Empirical Comparison of Supervised Learning Algorithms Using Different Performance Metrics. *23rd international conference on Machine Learning, 25-29 June 2006,  Pittsburgh, Pennsylvania, USA* [online]. New York: Association for Computing Machinery. pp. 161–168. [Viewed 16 August 2019]. Available from: doi: 10.1145/1143844.1143865

Flach, P., (2019). Performance Evaluation in Machine Learning: The Good, The Bad, The Ugly and The Way Forward. *33rd AAAI Conference on Artificial Intelligence, 27 January-1 February 2019, Honolulu, Hawaii, USA* [online]. Proceedings of the AAAI Conference on Artificial Intelligence. pp. 9808-9814. [Viewed 9 August 2019]. Available from: doi: 10.1609/aaai.v33i01.33019808

Hossin, M. and Sulaiman, M. N., (2015). A Review on Evaluation Metrics for Data Classification Evaluations. *International Journal of Data Mining & Knowledge Management Process* [online]. 5(2), 01–11. [Viewed 11 August 2019]. Available from: doi: 10.5121/ijdkp.2015.5201

Iraji, M. S., (2019). Comparison between soft computing methods for tomato quality grading using machine vision. *Journal of Food Measurement and Characterization* [online]. **13**(1), 1–15. [Viewed 15 August 2019]. Available from: doi: 10.1007/s11694-018-9913-2

Japkowicz, N. (2006). Why Question Machine Learning Evaluation Methods?. In: the American Association for Artificial Intelligence (AAAI).  *AAAI-2006 Workshop on Evaluation Methods for Machine Learning,16-17July 2006, Boston, Massachusetts, USA* [online]. Boston: AAAI Press, p. 6. [Viewed 9 August 2019]. Available from: URL: https://www.aaai.org/Papers/Workshops/2006/WS-06-06/WS06-06-003.pdf

Kaplan, A. and Haenlein, M., (2019). Siri, Siri, in my hand: Who’s the fairest in the land? *On the interpretations, illustrations, and implications of artificial intelligence* [online]. **62**(1), 15–25. [Viewed 7 August 2019]. Available from: doi: 10.1016/j.bushor.2018.08.004

Liu, Y., Zhou, Y., Wen, S. and Tang, C., (2014). A Strategy on Selecting Performance Metrics for Classifier Evaluation. *International Journal of Mobile Computing and Multimedia Communications* [online], **6**(4), 20–35. [Viewed 16 August 2019]. Available from: doi: 10.4018/IJMCMC.2014100102

McCarthy, J. et al., (2006). A proposal for the Dartmouth summer research project on artificial intelligence: August 31, 1955. *AI Magazine* [online]. **27**(4), 12–14. [Viewed 7 August 2019]. Available from: doi: 10.1609/aimag.v27i4.1904

Moor, J. H., (1976). AN ANALYSIS OF THETURING TEST. *Phliosophical Studies*[online]. 30(4), 249–257. [Viewed 9 August 2019]. Available from: URL: https://link.springer.com/content/pdf/10.1007%2FBF00372497.pdf

Nambiar, R. and Poess, M., (2010). Transaction performance vs. Moore’s law: a trend analysis. *Technology Conference on Performance Evaluation and Benchmarking, 13-17 September 2010, Singapore, Singapore* [online]. Berlin: Springer. pp. 110-120. [Viewed 13 August]. Available from: doi: ﻿10.1016/0020-7101(78)90038-7

Oord, A.V.D, *et al.*, 2016. Wavenet: A generative model for raw audio. [Viewed 9 August 2019]. Available from: arXiv:1609.03499.

Peter Stone, E., (2016). Artificial Intelligence and Life in 2030. *One Hundred Year Study on Artificial Intelligence: Report of the 2015-2016 Study Panel* [online]. Stanford: Stanford University. [Viewed 1 August 2019]. Available from: URL: https://ai100.stanford.edu/sites/default/files/ai_100_report_0901fnlc_single.pdf

Provost, F., Fawcett, T. and Kohavi, R., (1998). The Case Against Accuracy Estimation for Comparing Induction Algorithms. *15th  International Conference on Machine Learning (ICML '98), 24-27 July 1998, Madison, Wisconsin, USA* [online]. San Francisco: Morgan Kaufmann. pp. 445-453. [Viewed 14 August]. Available from: URL: https://pdfs.semanticscholar.org/7770/3a2783f64dfceb638aa9eebd9c9c501bb835.pdf

Turing, A. M., (1950). Computing Machinery and Intelligence. *Mind* [online]. **59**(236), 433–460. [Viewed 2 August]. Available from: URL: https://academic.oup.com/mind/article/LIX/236/433/986238

Weng, B., Ahmed, M. A. and Megahed, F. M., (2017). *Stock market one-day ahead movement prediction using disparate data sources, Expert Systems with Applications* [online]. **79**, 153–163. [Viewed 15 August]. Available from: doi: 10.1016/j.eswa.2017.02.041

Wong, S., (2018). Should you let google's ai book your haircut? ﻿*New Scientist* [online]. **238**(3178), 21. [Viewed 10 August 2019]. Available from: doi: ﻿10.1016/s0262-4079(18)30882-0