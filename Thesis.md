## Describe some recent developments in Artificial Intelligence (AI) and evaluate some of the ways of testing its reliability

Artificial Intelligence was first mentioned in the Dartmouth Summer Research Project. Initially, it was cursorily defined as a machine runs in a brainy way, the way like a person acts (J. McCarthy, 1995). Now, Andreas Kaplan delicately defines AI as a system's capability to construe the input data correctly,  to learn from data and achieve particular goals and tasks by using this learning (Andreas Kaplan, 2019). In fact, AI performs well in the lab, but maybe overwhelmed when it encounters unknown data. Therefore, it is necessary to explore the reliability of AI. Regarding reliability testing, AI has many evaluation metrics such as threshold, MSE and AUC. Although threshold is a method commonly used by researchers, this paper suggests AUC is more suitable for testing AI reliability. In this study, recent developments of AI will be discussed. Then methods commonly used for testing will be explained and evaluated. Finally conclusions and limitations will be given in the end.

AI has a vigorous development and application in various fields.
**Stock market prediction:** Due to online trading and massive data collection, stock market forecasting has become possible. Specifically, AI is now able to predict future trends by learning stock market data. Recently, a financial expert system designed by Bin Weng (Bin Weng, 2017), through learning stock market sequences and “knowledge bases” such as Google and Wikipedia, is able to successfully predict the AAPL stock movement in the one-day ahead with an accuracy of 85%. While the algorithm may be correct, Weng's study is incomplete because they do not consider other sources of stock information. Henrique suggests that Weng's methods for obtaining, interpreting, and applying relevant data need to be explored. (B. Henrique, 2019). In addition, the reliability of the data sources is also suspectable because everyone can add information to Google and Wikipedia, and using such data may introduce false data and affect AI performance, or even get the opposite results compared with the ground truth. Batra argues despite Weng's accuracy of 85%, they don't define any mechanism to verify the external data, such as the data from Google and Wikipedia which can be unsubstantiated (Batra, 2018). Thus, although the stock market forecasting algorithm involving AI can achieve higher accuracy, a more comprehensive and reliable stock market database needs to be collected.**Target tracking:** Similarly, the object
tracking algorithm proposed by Li Bo Chang also combines time series and CNNs,
which greatly improves the robustness of object occlusion. This
innovative idea combines CNN's ability to learn feature representations with
time continuity and solves a series of problems that are difficult to track due
to external changes, such as blur, occlusion, and background clutter. At the
same time, the accuracy of tracking reached 85%, and the AUC score (one of the
popular ranking type indicators) increased by 14.9% compared to the traditional
KCF method.
**Tomato quality grading:** AI also has a positive impact on agricultural development. Mohammad Saber Iraji assists agricultural production by designing high-accuracy intelligent systems using different artificial intelligence methods. Her well-designed intelligent system can accurately score and classify tomato quality, achieving 95.5% accuracy, and performance is much better than other algorithms.

The Turing test does not necessarily test the reliability of the AI. And there is no clear literature showing a specific correlation between the Turing test and AI stability. It seems possible that these results are due to the aim of the Turing Test is to imitate human rather than evaluating the quality of AI. According to Turing's study, he raised a question of whether machine can think and tries to convert this problem in a more concrete way and proposed an imitation game (Turing, 1950). Additionally, the Turing test has not been quantified and scored for AI reliability, more is cognitive research. Moor also asserts Turing test has little value in guiding research (Moor, 1976). 

Therefore, evaluation metrics, a more quantitative way, are needed to assess the reliability of intelligent systems. These metrics are suitable for assessing the reliability of an intelligent system. According to Hossin, the reliability of AI can be measured by the evaluation metric, which can be seen as a measurement tool. Evaluation metrics can be divided into three main categories. In order to more easily compare the performance of various evaluation metrics, in Caruana's research, he astutely divided the nine common evaluation metrics into three groups: threshold metrics, ordering/rank metrics, and probability metrics.

In the above evaluation metrics, accuracy as one of the popular threshold metric is often used as the gold standard for researchers. More and more researchers use it as a standard for evaluating their own AI models because of its convenience. As Hossin observe, the threshold and ranking metric are the methods most commonly used by researchers to measure and summarize the ability of trained classifier when dealing with new data (Hossin, M, 2015). In Japkowicz review of the accuracy, he claims that researchers compare different intelligent system based on accuracy which counts the number of mistakes. This is the simplest and most directive evaluation measure (Japkowicz, 2006). However, there is a limit to only use accuracy as an evaluation criterion. Provost brings severe concerns about the use of accuracy. In his convincing work, he points out that accuracy is unsatisfactory both for making practical choices and for drawing scientific conclusions (Provost, 2011). Japkowicz (Japkowicz, 2006) mentions another argument for the accuracy's assessment ability is limited. He emphasizes the accuracy does not distinguish between the types of errors it makes, and in some cases ignoring the problem can lead to catastrophic consequences, such as cancer discrimination. Thus, accuracy can not be the only one to evaluate the testing reliability of the AI system.