## Describe some recent developments in Artificial Intelligence (AI) and evaluate some of the ways of testing its reliability

Artificial Intelligence was first mentioned in the Dartmouth Summer Research Project. Initially, it was cursorily defined as a machine runs in a brainy way, the way like a person acts (J. McCarthy, 1995). Now, Andreas Kaplan delicately defines AI as a system's capability to construe the input data correctly,  to learn from data and achieve particular goals and tasks by using this learning (Andreas Kaplan, 2019). In fact, AI performs well in the lab, but maybe overwhelmed when it encounters unknown data. Therefore, it is necessary to explore the reliability of AI. Regarding reliability testing, AI has many evaluation metrics such as threshold, MSE and AUC. Although threshold is a method commonly used by researchers, this paper suggests AUC is more suitable for testing AI reliability. In this study, recent developments of AI will be discussed. Then methods commonly used for testing will be explained and evaluated. Finally conclusions and limitations will be given in the end.

AI has a vigorous development and application in various fields.
**Stock market prediction:** Due to online trading and massive data collection, stock market forecasting has become possible. Specifically, AI is now able to predict future trends by learning stock market data. Recently, a financial expert system designed by Bin Weng (Bin Weng, 2017), through learning stock market sequences and “knowledge bases” such as Google and Wikipedia, is able to successfully predict the AAPL stock movement in the one-day ahead with an accuracy of 85%. While the algorithm may be correct, Weng's study is incomplete because they do not consider other sources of stock information. Henrique suggests that Weng's methods for obtaining, interpreting, and applying relevant data need to be explored. (B. Henrique, 2019). In addition, the reliability of the data sources is also suspectable because everyone can add information to Google and Wikipedia, and using such data may introduce false data and affect AI performance, or even get the opposite results compared with the ground truth. Batra argues despite Weng's accuracy of 85%, they don't define any mechanism to verify the external data, such as the data from Google and Wikipedia which can be unsubstantiated (Batra, 2018). Thus, although the stock market forecasting algorithm involving AI can achieve higher accuracy, a more comprehensive and reliable stock market database needs to be collected.**Target tracking:** Similarly, the object
tracking algorithm proposed by Li Bo Chang also combines time series and CNNs,
which greatly improves the robustness of object occlusion. This
innovative idea combines CNN's ability to learn feature representations with
time continuity and solves a series of problems that are difficult to track due
to external changes, such as blur, occlusion, and background clutter. At the
same time, the accuracy of tracking reached 85%, and the AUC score (one of the
popular ranking type indicators) increased by 14.9% compared to the traditional
KCF method (Chang, 2019).
**Tomato quality grading:** AI also has a positive impact on agricultural development. Iraji assists agricultural production by designing high-accuracy intelligent systems using different artificial intelligence methods. Her well-designed intelligent system can accurately score and classify tomato quality, achieving 95.5% accuracy, and performance is much better than other algorithms (Iraji, 2019).

The Turing test does not necessarily test the reliability of the AI. And there is no clear literature showing a specific correlation between the Turing test and AI stability. It seems possible that these results are due to the aim of the Turing Test is to imitate human rather than evaluating the quality of AI. According to Turing's study, he raised a question of whether machine can think and tries to convert this problem in a more concrete way and proposed an imitation game (Turing, 1950). Additionally, the Turing test has not been quantified and scored for AI reliability, more is cognitive research. Moor also asserts Turing test has little value in guiding research (Moor, 1976). 

Therefore, evaluation metrics, a more quantitative way, are needed to assess the reliability of intelligent systems. These metrics are suitable for assessing the reliability of an intelligent system. According to Hossin, the reliability of AI can be measured by the evaluation metric, which can be seen as a measurement tool (Hossin, M, 2015). Evaluation metrics can be divided into three main categories. In order to more easily compare the performance of various evaluation metrics, in Caruana's research, he astutely divided the nine common evaluation metrics into three groups: threshold metrics, probability metrics, and ordering/rank metrics (Caruana, 2004).

In the above evaluation metrics, accuracy as one of the popular threshold metric is often used as the gold standard for researchers. More and more researchers use it as a standard for evaluating their own AI models because of its convenience. As Hossin observe, the threshold and ranking metric are the methods most commonly used by researchers to measure and summarize the ability of trained classifier when dealing with new data (Hossin, M, 2015). In Japkowicz review of the accuracy, he claims that researchers compare different intelligent system based on accuracy which counts the number of mistakes. This is the simplest and most directive evaluation measure (Japkowicz, 2006). However, there is a limit to only use accuracy as an evaluation criterion. Provost brings severe concerns about the use of accuracy. In his convincing work, he points out that accuracy is unsatisfactory both for making practical choices and for drawing scientific conclusions (Provost, 2011). Japkowicz mentions another argument for the accuracy's assessment ability is limited. He emphasizes the accuracy does not distinguish between the types of errors it makes, and in some cases ignoring the problem can lead to catastrophic consequences, such as cancer discrimination (Japkowicz, 2006). Thus, accuracy can not be the only one to evaluate the testing reliability of the AI system.

The probability metrics are mainly determined by the predicted value, it is a measure of the difference between the predicted value and the ground truth. Probability metrics give the most intuitive gap. According to Liu's research, probability metrics are widely used in regression problems by calculating the bias and can be used to evaluate the reliability of intelligent systems. But he also points out that unlike other metrics, It cannot be compared directly to threshold metrics nor the ranking metrics (Liu, 2014). Probability metrics can still not effectively handle the task of assessing the reliability. In Hossin's case study of probability metrics, he states that like the defect of accuracy, the probability metrics cannot effectively provide information between different classes. He also wisely realized that the probability metrics depends to a large extent on the weight of the initialization (Hossin, M, 2015). In some cases, the probability metrics may not matter. According to Caruana's finding, he reveals that in the case where the two evaluation metrics, threshold and ranking, perform extremely well, probability metrics may not perform well (Caruana, 2006).

The area under the curve (AUC) can be used as an indicator to evaluate the stability of intelligent systems. It's not as simple as accuracy, it reflects the overall performance of the system. In Flach's thoughtful research, he points out that it makes sense to use the area under the curve as a comprehensive performance indicator (Flach, 2019). Similar, Japkowicz emphasizes, unlike other metrics, its great advantage is that it separates the algorithm’s performance of each class, avoiding the shortcomings of the two evaluation metrics mentioned above (Japkowicz, 2006). Additionally, Hossin mentions the area under the curve (AUC) reflects the overall performance of the AI system. Nevertheless, he also accept that although AUC is very good at evaluation and discrimination, the cost of calculating AUC is too high. However, considering its excellent ability in evaluating the reliability of an ai system, in today’s era of doubling computing power annually, such high computing consumption is affordable.

Two interesting points are mentioned. On the one hand, in Flach's research, he suggests the ability of ai system and the difficulty of the relationship between data are not likely to be observed directly which indicates the needs for latent-variable models and causal inference(Flach, 2019). On the other hand, Liu's finding shows that excellent ai system does not necessarily need to achieve the best in every evaluation metric, as long as the system meets the performance requirements of an application(Liu, 2014). Thus, ai system reliability measurement needs to be further explored.

In conclusion, these results suggest that the area under the curve (AUC) might be more suitable for testing AI reliability at present. This essay objectively analyzes three types of assessment metrics used to test AI reliability. Their advantages, disadvantages and reliability are analyzed separately. However, it still needs further investigation because it does not cover all aspects of testing the reliability of an intelligent system. Therefore, it may not be possible to take into account the stability of the AI test in other aspects.